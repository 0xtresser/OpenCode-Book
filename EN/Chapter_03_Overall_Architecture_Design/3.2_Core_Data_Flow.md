# 3.2 Core Data Flow: From User Input to AI Response

> **Model**: claude-opus-4-6 (anthropic/claude-opus-4-6)
> **Generation Date**: 2025-02-17

---

The key to understanding OpenCode's architecture is understanding how data flows from a single keyboard input by the user through the entire system, ultimately producing an AI response. This section traces that data flow in full.

## 3.2.1 User Message -> Session.chat() -> SessionPrompt.prompt() -> LLM.stream()

When a user types a message in the TUI and presses Enter, the following call chain is triggered in sequence:

```
User presses Enter
    |
    v
TUI sends HTTP POST request
POST /project/:id/session/:sid/message
body: { parts: [{ type: "text", text: "Help me implement a sort function" }] }
    |
    v
Server route handler (routes/session.ts)
    |
    v
SessionPrompt.prompt(input)          <- session/prompt.ts
    |
    +-- 1. Session.get(sessionID)     Retrieve session info
    +-- 2. SessionRevert.cleanup()    Clean up incomplete reverts
    +-- 3. createUserMessage(input)   Create user message and store it
    +-- 4. Session.touch(sessionID)   Update session timestamp
    +-- 5. loop({ sessionID })        Enter the Agentic Loop <-- Core!
```

The source code structure of `SessionPrompt.prompt()` clearly illustrates this flow:

```typescript
export const prompt = fn(PromptInput, async (input) => {
  const session = await Session.get(input.sessionID)
  await SessionRevert.cleanup(session)
  const message = await createUserMessage(input)
  await Session.touch(input.sessionID)

  if (input.noReply === true) return message

  return loop({ sessionID: input.sessionID })
})
```

The `loop()` function is the entry point for the entire Agentic Loop. It will:

1. Load all messages for the session from the database
2. Determine the current Agent and Model to use
3. Assemble the System Prompt (including AGENTS.md instructions, environment info, etc.)
4. Register all available Tools
5. Create a `SessionProcessor`
6. Call `LLM.stream()` to initiate a streaming LLM call
7. Process the streaming response (text, chain-of-thought, tool calls)

## 3.2.2 Streaming Response Handling: The SessionProcessor's Agentic Loop

`SessionProcessor` (`session/processor.ts`) is one of the most complex and important modules in OpenCode. It implements the complete Agentic Loop:

```typescript
export function create(input: {
  assistantMessage: MessageV2.Assistant
  sessionID: string
  model: Provider.Model
  abort: AbortSignal
}) {
  let attempt = 0
  let needsCompaction = false

  return {
    async process(streamInput: LLM.StreamInput) {
      while (true) {  // <- The Agentic Loop's outer loop
        const stream = await LLM.stream(streamInput)

        for await (const value of stream.fullStream) {
          switch (value.type) {
            case "reasoning-start":     // Chain-of-thought starts
            case "reasoning-delta":     // Chain-of-thought increment
            case "reasoning-end":       // Chain-of-thought ends
            case "text-delta":          // Text increment
            case "tool-call-start":     // Tool call starts
            case "tool-call-delta":     // Tool call argument increment
            case "tool-call-complete":  // Tool call complete -> execute tool
            case "finish":              // This round of generation is complete
          }
        }

        // Check if there are tool calls to process
        // If yes -> execute tools -> append results to messages -> continue loop
        // If no  -> exit loop, AI response is complete
      }
    }
  }
}
```

**Processing order of streaming events**:

```
LLM starts generating
    |
    +-- reasoning-start    -> Create ReasoningPart
    +-- reasoning-delta    -> Update ReasoningPart.text (append increment)
    +-- reasoning-end      -> Complete ReasoningPart
    |
    +-- text-delta         -> Append to TextPart.text
    |
    +-- tool-call-start    -> Create ToolPart (status: pending)
    +-- tool-call-delta    -> Update ToolPart arguments
    +-- tool-call-complete -> Mark ToolPart (status: running) -> execute tool
    |
    +-- finish             -> This round is complete, check if continuation is needed
```

Every Part change is written to storage in real time via `Session.updatePart()` and pushed to the frontend through the event bus, enabling real-time display.

## 3.2.3 Tool Call Loop: Tool Call -> Execute -> Result -> Continue

When an LLM response contains a tool call, the processing flow is as follows:

```
LLM returns a tool call request
e.g.: { tool: "read", args: { filePath: "/src/index.ts" } }
    |
    v
1. Permission check (PermissionNext.evaluate)
    |
    +-- allow -> Continue execution
    +-- ask   -> Pause, wait for user confirmation
    +-- deny  -> Deny execution, return error
    |
    v
2. Execute tool (Tool.execute)
    |
    +-- Snapshot.track()    -> Create file snapshot before execution
    +-- tool.execute(args)  -> Actual execution
    +-- Write result to ToolPart -> Status becomes completed/error
    |
    v
3. Return result to LLM
    The tool's output (e.g., file contents) is appended
    as a new tool_result message to the message list
    |
    v
4. LLM continues generating
    With the tool result, the LLM decides:
    +-- Call more tools -> Go back to step 1
    +-- Generate a text reply -> Exit the loop
    +-- Trigger Doom Loop detection
```

**Doom Loop Detection**:

OpenCode has a built-in Doom Loop (infinite loop) detection mechanism:

```typescript
const DOOM_LOOP_THRESHOLD = 3
```

If the Agent executes the same operation pattern 3 consecutive times without making substantive progress, the system triggers a `doom_loop` permission check. Depending on the configuration, this may pause execution and ask the user whether to continue. This is a safety measure to prevent the AI from getting stuck in an infinite loop.

## 3.2.4 Complete Request-Response Sequence Diagram

Putting all the above steps together, the timing of a complete conversation interaction is as follows:

```
  User         TUI          Server       SessionPrompt    LLM      Tool
   |            |             |               |            |         |
   |--input---->|             |               |            |         |
   |            |--POST msg-->|               |            |         |
   |            |             |--prompt()---->|            |         |
   |            |             |               |            |         |
   |            |             |               |--stream()->|         |
   |            |<-SSE:text---|<-Bus.publish--|<-text-d----|         |
   |            |<-SSE:text---|<-Bus.publish--|<-text-d----|         |
   |            |             |               |            |         |
   |            |             |               |<-tool-call-|         |
   |            |<SSE:tool-st-|<-Bus.publish--|            |         |
   |            |             |               |--execute()--------->|
   |            |             |               |            | read   |
   |            |<SSE:tool-ok-|<-Bus.publish--|<-result----------<-|
   |            |             |               |            |         |
   |            |             |               |--stream()->| w/result|
   |            |<-SSE:text---|<-Bus.publish--|<-text-d----| cont.   |
   |            |             |               |<-finish----|         |
   |            |<-SSE:done---|<-Bus.publish--|            |         |
   |            |             |               |            |         |
   |<-display---|             |               |            |         |
```

This sequence diagram illustrates OpenCode's core characteristics:

1. **Asynchronous streaming**: From the LLM's very first token, the user can see the response
2. **Event-driven**: All state changes are broadcast via the Bus, and the frontend receives them via SSE
3. **Loop architecture**: Tool calls and LLM generation alternate until the LLM decides to stop

Understanding this core data flow provides a coordinate system for the in-depth analysis of each module in subsequent chapters.
