# 6.4 System Prompt Architecture

> **Model**: claude-opus-4-6 (anthropic/claude-opus-4-6)
> **Generation Date**: 2026-02-17

---

In the Agent system, the System Prompt is the LLM's "operations manual" -- it defines the Agent's identity, behavioral norms, tool usage strategies, and output style. OpenCode's System Prompt architecture is not simply "a block of text," but rather a carefully designed **layered assembly system** that dynamically constructs the final Prompt based on different LLM Providers, Agent configurations, and session states.

> **Extended Explanation: What is a System Prompt?**
>
> In the LLM's conversation interface, messages are divided into three roles: `system` (system instructions), `user` (user input), and `assistant` (AI responses). The System Prompt is the instruction text sent to the LLM first, setting the behavioral tone for the AI throughout the entire conversation. Unlike User messages, System Prompts are typically written by application developers rather than end users, and are used to control the AI's "persona" and behavioral boundaries.
>
> In OpenCode, the System Prompt can be thousands of tokens long, containing detailed tool usage instructions, security specifications, output format requirements, and more.

## 6.4.1 `SystemPrompt.provider()` -- Selecting Prompt Templates by Model

Different LLMs have different "comprehension abilities" and "instruction-following" characteristics. Claude excels at following complex structured instructions, the GPT series responds differently to certain instruction formats, and Gemini has its own peculiarities. OpenCode uses the `SystemPrompt.provider()` function to select the most suitable Prompt template for each model family:

```typescript
// session/system.ts
export namespace SystemPrompt {
  export function provider(model: Provider.Model) {
    if (model.api.id.includes("gpt-5"))      return [PROMPT_CODEX]
    if (model.api.id.includes("gpt-") ||
        model.api.id.includes("o1") ||
        model.api.id.includes("o3"))          return [PROMPT_BEAST]
    if (model.api.id.includes("gemini-"))     return [PROMPT_GEMINI]
    if (model.api.id.includes("claude"))      return [PROMPT_ANTHROPIC]
    if (model.api.id.toLowerCase()
        .includes("trinity"))                 return [PROMPT_TRINITY]
    return [PROMPT_ANTHROPIC_WITHOUT_TODO]
  }
}
```

### Model-to-Prompt Mapping

| Model Family | Prompt File | Core Characteristics |
|-------------|------------|---------------------|
| Claude series | `anthropic.txt` | Complete task management instructions (Todo tool integration), professional and objective style |
| GPT-4/O1/O3 | `beast.txt` | Emphasis on autonomous execution, web research, step planning |
| GPT-5 | `codex_header.txt` | Streamlined coding instructions, frontend design guidelines |
| Gemini | `gemini.txt` | Structured workflow, detailed security specifications |
| Trinity | `trinity.txt` | Streamlined CLI tool instructions |
| Other models | `qwen.txt` | Basic instruction set (no Todo support) |

This mapping strategy uses simple **string containment checks** (`includes()`) rather than exact matching. This means both `claude-3-opus-20240229` and `claude-sonnet-4-20250514` will match to `anthropic.txt`. This lenient matching approach ensures that even when new model versions are released (e.g., `claude-4-xxx`), as long as the name contains `"claude"`, the correct Prompt will be automatically used.

### Design Differences Across Prompt Templates

Let us compare the design philosophies of several key Prompt templates:

**`anthropic.txt` (Claude-specific)**

```
You are OpenCode, the best coding agent on the planet.

# Task Management
You have access to the TodoWrite tools to help you manage and plan tasks.
Use these tools VERY frequently to ensure that you are tracking your tasks
and giving the user visibility into your progress.

# Tool usage policy
- When doing file search, prefer to use the Task tool
  in order to reduce context usage.
- You should proactively use the Task tool with specialized agents
  when the task at hand matches the agent's description.
```

The Claude version of the Prompt emphasizes:
- **Frequent use of Todo tools** -- Claude is considered proficient at following structured task management instructions.
- **Task delegation** -- Encourages Claude to proactively delegate search tasks to sub-Agents, reducing context consumption for the primary Agent.
- **Professional objectivity** -- Explicitly requires "technical accuracy takes priority over validating user beliefs."

**`beast.txt` (GPT series-specific)**

```
You are opencode, an agent - please keep going until the user's query
is completely resolved, before ending your turn and yielding back to the user.

You MUST iterate and keep going until the problem is solved.

THE PROBLEM CAN NOT BE SOLVED WITHOUT EXTENSIVE INTERNET RESEARCH.

You must use the webfetch tool to recursively gather all information
from URLs provided to you by the user...
```

The GPT version of the Prompt has a markedly different style:
- **Emphasis on continuous execution** -- GPT series models sometimes stop prematurely, so the Prompt repeatedly emphasizes "keep going."
- **Web research priority** -- Assumes GPT's knowledge may be outdated, requiring extensive use of the `webfetch` tool to verify information.
- **Memory mechanism** -- Introduces a `.github/instructions/memory.instruction.md` file to persist user preferences.
- **Casual tone** -- Uses more colloquial expressions (`"Whelp - I see we have some problems"`).

**`gemini.txt` (Gemini-specific)**

```
You are opencode, an interactive CLI agent specializing
in software engineering tasks.

# Core Mandates
- Conventions: Rigorously adhere to existing project conventions.
- Libraries/Frameworks: NEVER assume a library/framework is available.
  Verify its established usage.
- Style & Structure: Mimic the style, structure, framework choices,
  typing, and architectural patterns.
```

The Gemini version of the Prompt features:
- **Strict convention adherence** -- Gemini is considered prone to introducing libraries or frameworks not present in the project.
- **Structured workflow** -- Defines a complete five-step workflow (Understand -> Plan -> Implement -> Verify Tests -> Verify Standards).
- **Minimal output** -- "aim for fewer than 3 lines of text output per response."
- **New application development flow** -- The Gemini version specifically includes a detailed "create application from scratch" process guide.

**`qwen.txt` (Fallback version)**

For other models not in the above model families (such as Qwen, LLaMA, etc.), OpenCode uses a basic version. Compared to `anthropic.txt`, it **removes Todo tool-related instructions** -- because not all models can reliably use structured Todo management.

## 6.4.2 `SystemPrompt.environment()` -- Runtime Environment Information Injection

Beyond behavioral instructions, the System Prompt also needs to inform the LLM about the current runtime environment:

```typescript
export async function environment(model: Provider.Model) {
  const project = Instance.project
  return [
    [
      `You are powered by the model named ${model.api.id}. ` +
      `The exact model ID is ${model.providerID}/${model.api.id}`,
      `Here is some useful information about the environment you are running in:`,
      `<env>`,
      `  Working directory: ${Instance.directory}`,
      `  Is directory a git repo: ${project.vcs === "git" ? "yes" : "no"}`,
      `  Platform: ${process.platform}`,
      `  Today's date: ${new Date().toDateString()}`,
      `</env>`,
      `<directories>`,
      `  ${/* directory tree info (currently disabled) */ ""}`,
      `</directories>`,
    ].join("\n"),
  ]
}
```

This code injects the following runtime information:

| Information | Description | Purpose |
|-------------|-------------|---------|
| Model ID | Current model identifier | Lets the Agent know "who it is"; sometimes affects self-assessment behavior |
| Working directory | Project working directory | Agent needs to know the project root to construct correct file paths |
| Git repo | Whether it is a Git repository | Affects whether the Agent suggests using Git commands |
| Platform | Operating system | Affects shell command selection (e.g., `ls` vs `dir`) |
| Date | Current date | LLMs have a training data cutoff date; providing the current date helps it judge information timeliness |

Notably, the `<directories>` section is currently disabled (`false` condition). It was originally planned to use `Ripgrep.tree()` to inject the project directory structure. This is likely for performance or token consumption reasons -- a large project's directory tree could consume a significant number of tokens.

## 6.4.3 Layered Prompt Assembly Logic

The final assembly of the System Prompt occurs in the `LLM.stream()` function. Let us trace the complete assembly flow:

```typescript
// llm.ts
export async function stream(input: StreamInput) {
  const system = []

  // Step 1: Determine the base Prompt
  system.push(
    [
      // Prefer Agent's custom Prompt; otherwise use Provider default Prompt
      // Codex mode skips Provider Prompt (sent via options.instructions)
      ...(input.agent.prompt
        ? [input.agent.prompt]
        : isCodex
          ? []
          : SystemPrompt.provider(input.model)),

      // Additional system prompts (from the caller)
      ...input.system,

      // System prompt carried by the user message
      ...(input.user.system ? [input.user.system] : []),
    ]
      .filter((x) => x)
      .join("\n"),
  )

  // Step 2: Save original Prompt for fallback
  const header = system[0]
  const original = clone(system)

  // Step 3: Plugin transformation
  await Plugin.trigger(
    "experimental.chat.system.transform",
    { sessionID: input.sessionID, model: input.model },
    { system },
  )

  // If Plugin emptied the system, restore original values
  if (system.length === 0) {
    system.push(...original)
  }

  // Step 4: Optimize cache structure
  // Maintain a 2-part structure: [header, rest]
  // This helps with Anthropic API's prompt caching
  if (system.length > 2 && system[0] === header) {
    const rest = system.slice(1)
    system.length = 0
    system.push(header, rest.join("\n"))
  }

  // Step 5: Send as system messages
  return streamText({
    messages: [
      ...system.map((x) => ({
        role: "system",
        content: x,
      })),
      ...input.messages,  // User and assistant conversation history
    ],
    // ...
  })
}
```

### Prompt Selection Priority

```
Agent.prompt  >  SystemPrompt.provider()  >  (none)
```

1. **If the Agent has a custom Prompt** (e.g., `explore`, `compaction`, etc.), that Prompt is used directly.
2. **If the Agent has no custom Prompt** (e.g., `build`), `SystemPrompt.provider()` is used to select the corresponding Prompt template based on the model.
3. **Codex mode exception**: In Codex mode (OpenAI OAuth), the Provider Prompt is sent via `options.instructions` rather than as a system message.

### Plugin Transformation Layer

```typescript
await Plugin.trigger(
  "experimental.chat.system.transform",
  { sessionID: input.sessionID, model: input.model },
  { system },
)
```

This is one of the key hooks in the Plugin system. Plugins can modify the Prompt before it is sent to the LLM -- adding extra instructions, replacing certain parts, or completely rewriting it. oh-my-opencode heavily uses this hook to inject its complex Agent behavioral instructions.

The system includes a **safety fallback**: if a Plugin accidentally empties the `system` array, it restores the original Prompt. This defensive programming ensures that even if a Plugin malfunctions, the Agent will not run without any System Prompt (which would lead to unpredictable behavior).

### Prompt Cache Optimization

```typescript
if (system.length > 2 && system[0] === header) {
  const rest = system.slice(1)
  system.length = 0
  system.push(header, rest.join("\n"))
}
```

> **Extended Explanation: Prompt Caching**
>
> Providers like Anthropic offer Prompt Caching functionality -- if multiple consecutive calls use the same System Prompt prefix, the API can cache the already-processed portion, reducing latency and cost. OpenCode maximizes cache hit rates by maintaining the System Prompt in a fixed two-part structure `[header, rest]`:
>
> - `header` (first part) is typically the unchanging base Prompt (e.g., `anthropic.txt`), which remains consistent throughout the session.
> - `rest` (second part) may contain dynamic content (e.g., environment information, Plugin-injected instructions).
>
> This way, as long as the header part remains unchanged, the API can cache its processing results, significantly reducing latency for subsequent requests.

### Complete System Prompt Structure

Combining the analysis above, the complete System Prompt structure for a single LLM call is as follows:

```
+---------------------------------------------+
|  System Message 1: Header                    |
|  +-------------------------------------+    |
|  |  Provider Prompt / Agent Prompt      |    |
|  |  (anthropic.txt / explore.txt / ...) |    |
|  +-------------------------------------+    |
+---------------------------------------------+
|  System Message 2: Dynamic Content           |
|  +-------------------------------------+    |
|  |  Additional system instructions      |    |
|  |  + User message system prompt        |    |
|  |  + Plugin-injected content           |    |
|  |  + Environment info (model,          |    |
|  |    directory, date...)               |    |
|  |  + Instruction file contents         |    |
|  +-------------------------------------+    |
+---------------------------------------------+
|  User/Assistant Messages: Conversation       |
|  History                                     |
|  +-------------------------------------+    |
|  |  [user] "Please refactor this        |    |
|  |         function for me"             |    |
|  |  [assistant] "Sure, let me first     |    |
|  |              analyze..."             |    |
|  |  [tool-call] read("src/auth.ts")     |    |
|  |  [tool-result] { content: "..." }    |    |
|  |  ...                                 |    |
|  +-------------------------------------+    |
+---------------------------------------------+
```

## 6.4.4 Provider-Specific Option Injection

Beyond the Prompt, LLM calls also involve injection of Provider-specific options:

```typescript
const params = await Plugin.trigger(
  "chat.params",
  { sessionID, agent, model, provider, message },
  {
    temperature: input.model.capabilities.temperature
      ? (input.agent.temperature ?? ProviderTransform.temperature(input.model))
      : undefined,
    topP: input.agent.topP ?? ProviderTransform.topP(input.model),
    topK: ProviderTransform.topK(input.model),
    options,
  },
)
```

The parameter priority chain is:

```
Agent-level  >  Model-level  >  Provider default-level
```

For example, if the `title` Agent sets `temperature: 0.5`, this value overrides the model and Provider default temperature settings. This hierarchical parameter management ensures each Agent can precisely control the LLM's behavior.

## 6.4.5 Tool Resolution and Permission Filtering

In the final step before an LLM call, the system filters available tools based on the Agent's permission rules:

```typescript
async function resolveTools(input: Pick<StreamInput, "tools" | "agent" | "user">) {
  const disabled = PermissionNext.disabled(Object.keys(input.tools), input.agent.permission)
  for (const tool of Object.keys(input.tools)) {
    if (input.user.tools?.[tool] === false || disabled.has(tool)) {
      delete input.tools[tool]
    }
  }
  return input.tools
}
```

This code performs two layers of filtering:

1. **Permission rule filtering**: Based on the Agent's `permission` field, tools that are `deny`-ed are removed.
2. **User message-level filtering**: Users can temporarily disable certain tools within a single message (`input.user.tools`).

The filtered tool list is passed to `streamText()` -- the LLM can only "see" and invoke tools that are permitted. This ensures that even if the LLM attempts to call a prohibited tool, the request will be rejected.

---

This section provided a complete analysis of the System Prompt architecture design -- from model selection to layered assembly, from Plugin transformation to cache optimization. In the next section, we will turn to custom Agent configuration methods, understanding how users can create their own Agents through configuration files and directory structures.
