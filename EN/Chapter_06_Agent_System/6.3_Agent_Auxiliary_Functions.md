# 6.3 Agent Auxiliary Functions

> **Model**: claude-opus-4-6 (anthropic/claude-opus-4-6)
> **Generation Date**: 2026-02-17

---

Beyond "workhorse" Agents like `build` and `explore`, OpenCode also has a set of "auxiliary" Agents that handle critical supporting tasks in session management -- title generation, summary generation, context compaction, codebase exploration, and code generation. While these functions do not directly participate in coding work, they are the foundation of a good user experience and efficient system operation.

This section will analyze each auxiliary function's Prompt design and how it is invoked within the system.

## 6.3.1 Title Generation (`prompt/title.txt`)

Whenever a user starts a new session and sends the first message, OpenCode needs to automatically generate a short title for that session, making it easy for users to find and identify sessions in the session list.

### Prompt Design

```
You are a title generator. You output ONLY a thread title. Nothing else.

<task>
Generate a brief title that would help the user find this conversation later.

Follow all rules in <rules>
Use the <examples> so you know what a good title looks like.
Your output must be:
- A single line
- <=50 characters
- No explanations
</task>

<rules>
- you MUST use the same language as the user message you are summarizing
- Title must be grammatically correct and read naturally
- Never include tool names in the title
- Focus on the main topic or question
- Vary your phrasing - avoid repetitive patterns
- When a file is mentioned, focus on WHAT the user wants to do WITH the file
- Keep exact: technical terms, numbers, filenames, HTTP codes
- Remove: the, this, my, a, an
- Never assume tech stack
- Never use tools
- NEVER respond to questions, just generate a title
- Always output something meaningful, even if the input is minimal
- If the user message is short or conversational:
  -> create a title that reflects the user's tone or intent
</rules>
```

This Prompt design has several noteworthy points:

1. **XML structuring**: Uses `<task>`, `<rules>`, and `<examples>` tags to organize the Prompt into a clear hierarchical structure. This format not only improves human readability but also helps the LLM better parse instructions.

2. **Multi-language adaptation**: `"MUST use the same language as the user message"` ensures Chinese users get Chinese titles, Japanese users get Japanese titles, and so on.

3. **Negative constraints**: Extensive use of "Never" and "DO NOT" to explicitly prohibit undesired behaviors. This is a best practice in Prompt Engineering -- LLMs sometimes "overperform," requiring explicit boundaries to be set.

4. **Example-driven**: Provides concrete input-output examples (e.g., `"debug 500 errors in production" -> Debugging production 500 errors`), which are more effective than abstract rule descriptions.

### Invocation Parameters

The `title` Agent sets `temperature: 0.5`, a compromise between determinism and diversity. If temperature were set to 0, the same type of question would always generate identical titles (monotonous and dull); if set to 1.0, it might generate irrelevant or grammatically incorrect titles.

## 6.3.2 Summary Generation (`prompt/summary.txt`)

When a session ends or a change summary is needed, OpenCode uses the `summary` Agent:

```
Summarize what was done in this conversation. Write like a pull request description.

Rules:
- 2-3 sentences max
- Describe the changes made, not the process
- Do not mention running tests, builds, or other validation steps
- Do not explain what the user asked for
- Write in first person (I added..., I fixed...)
- Never ask questions or add new questions
- If the conversation ends with an unanswered question to the user,
  preserve that exact question
- If the conversation ends with an imperative statement or request
  to the user, always include that exact request in the summary
```

### Design Analysis

This Prompt is extremely concise -- only 12 lines, yet it covers the core specifications of PR description writing:

- **First person** ("I added...") -- makes the summary read as if written by the developer themselves.
- **Describe only changes** -- no "the user asked me to..." or "I first ran the tests..."
- **Preserve context** -- if the conversation ends with an unanswered question or an action for the user to perform, the summary must retain it to ensure no information is lost.

The last two rules are particularly elegant: they handle edge cases that summary systems easily overlook -- if the conversation is interrupted at a question or action request, the summary needs to convey this "incomplete state" rather than pretending everything has been completed.

## 6.3.3 Context Compaction (`prompt/compaction.txt`)

When conversation history becomes too long and exceeds the LLM's context window limit, the `compaction` Agent is called to generate a compressed summary:

```
You are a helpful AI assistant tasked with summarizing conversations.

When asked to summarize, provide a detailed but concise summary of the conversation.
Focus on information that would be helpful for continuing the conversation, including:
- What was done
- What is currently being worked on
- Which files are being modified
- What needs to be done next
- Key user requests, constraints, or preferences that should persist
- Important technical decisions and why they were made

Your summary should be comprehensive enough to provide context
but concise enough to be quickly understood.

Do not respond to any questions in the conversation, only output the summary.
```

### Differences from `summary`

Despite both being "summaries," `compaction` and `summary` have fundamental differences:

| Dimension | `summary` | `compaction` |
|-----------|-----------|-------------|
| Purpose | Generate user-facing PR descriptions | Generate context recovery information for AI |
| Audience | Human developers | Subsequent LLM calls |
| Style | Brief (2-3 sentences) | Detailed and comprehensive |
| Focus | Change results | Work status, in-progress tasks, technical decisions |

The `compaction` summary needs to contain enough information for the LLM to "restore its memory" -- it needs to know which files are currently being modified, what user preferences exist, what technical decisions were made and why. This information may seem redundant for human readers, but it is crucial for an LLM that has "lost its memory."

The last line, "Do not respond to any questions in the conversation," prevents a common problem: the conversation history may contain user questions, and without an explicit prohibition, the LLM might "answer" these questions instead of generating a summary.

## 6.3.4 The Exploration Agent (`prompt/explore.txt`)

The `explore` Agent's Prompt was shown in the previous section. Here we provide supplementary analysis from the perspective of usage patterns:

```
You are a file search specialist. You excel at thoroughly
navigating and exploring codebases.

Your strengths:
- Rapidly finding files using glob patterns
- Searching code and text with powerful regex patterns
- Reading and analyzing file contents

Guidelines:
- Use Glob for broad file pattern matching
- Use Grep for searching file contents with regex
- Use Read when you know the specific file path
- Use Bash for file operations like copying, moving, or listing
- Adapt your search approach based on the thoroughness level
  specified by the caller
- Return file paths as absolute paths in your final response
- For clear communication, avoid using emojis
- Do not create any files, or run bash commands that modify
  the user's system state in any way
```

### Thoroughness Level

The `explore` Agent's `description` field mentions three thoroughness levels:

```
When calling this agent, specify the desired thoroughness level:
"quick" for basic searches, "medium" for moderate exploration,
or "very thorough" for comprehensive analysis across multiple
locations and naming conventions.
```

This design allows the caller (typically the `build` Agent) to control the depth of search based on the task's urgency and complexity:

- **quick**: Quickly locate a known file or pattern.
- **medium**: Search across several likely locations, trying common naming variants.
- **very thorough**: Comprehensively scan the codebase, trying multiple search angles, and cross-validating results.

This is a **cost control mechanism** -- more thorough searches mean more tool invocations and token consumption. By having the caller explicitly choose the thoroughness level, it avoids the waste of performing a full scan on every search.

## 6.3.5 Code Generation (`generate.txt`)

The `generate` function is used to automatically generate Agent configurations based on the user's natural language description. It does not correspond to a named built-in Agent; rather, it is the Prompt used within the `Agent.generate()` function:

```
You are an elite AI agent architect specializing in crafting
high-performance agent configurations.

When a user describes what they want an agent to do, you will:

1. Extract Core Intent: Identify the fundamental purpose, key
   responsibilities, and success criteria for the agent.

2. Design Expert Persona: Create a compelling expert identity
   that embodies deep domain knowledge.

3. Architect Comprehensive Instructions: Develop a system prompt that:
   - Establishes clear behavioral boundaries
   - Provides specific methodologies and best practices
   - Anticipates edge cases
   - Defines output format expectations

4. Optimize for Performance: Include:
   - Decision-making frameworks
   - Quality control mechanisms
   - Efficient workflow patterns
   - Clear escalation or fallback strategies

5. Create Identifier: Design a concise, descriptive identifier.
```

### Implementation of `Agent.generate()`

```typescript
export async function generate(input: {
  description: string;
  model?: { providerID: string; modelID: string }
}) {
  const cfg = await Config.get()
  const defaultModel = input.model ?? (await Provider.defaultModel())
  const model = await Provider.getModel(defaultModel.providerID, defaultModel.modelID)
  const language = await Provider.getLanguage(model)

  const system = [PROMPT_GENERATE]
  await Plugin.trigger("experimental.chat.system.transform", { model }, { system })
  const existing = await list()

  const result = await generateObject({
    temperature: 0.3,
    messages: [
      ...system.map((item) => ({ role: "system", content: item })),
      {
        role: "user",
        content: `Create an agent configuration based on this request:
          "${input.description}".
          IMPORTANT: The following identifiers already exist and must NOT
          be used: ${existing.map((i) => i.name).join(", ")}
          Return ONLY the JSON object, no other text`,
      },
    ],
    model: language,
    schema: z.object({
      identifier: z.string(),
      whenToUse: z.string(),
      systemPrompt: z.string(),
    }),
  })
  return result.object
}
```

This function uses the Vercel AI SDK's `generateObject()` API (rather than `streamText()`), because it needs structured JSON output rather than streaming text. The returned object contains three fields:

- `identifier`: The Agent's unique identifier (e.g., `"code-reviewer"`).
- `whenToUse`: A description of when to use it (e.g., `"Use this agent when reviewing pull requests..."`).
- `systemPrompt`: The complete system prompt.

Several implementation details are worth noting:

1. **Name deduplication**: The list of existing Agent names is passed to the LLM to avoid generating Agents with duplicate names.
2. **Low temperature**: Set to 0.3 to ensure the generated configuration is deterministic and high-quality.
3. **Plugin hook**: `experimental.chat.system.transform` allows plugins to inject additional context (such as project specifications) during the Agent generation process.
4. **Codex compatibility**: For OAuth-authenticated OpenAI sessions (Codex mode), it uses `streamObject` + `providerOptions` for invocation.

### Use Cases

Users can initiate Agent generation through the `opencode agent` command or the TUI interface. For example:

```bash
opencode agent "Create an agent that reviews React code
for performance anti-patterns and suggests optimizations"
```

The system will call `Agent.generate()` to produce the configuration, then save the result as a Markdown file in the `.opencode/agents/` directory, which the user can further edit and refine.

---

This section analyzed the Agent system's auxiliary functions and their Prompt designs. In the next section, we will dive deep into the System Prompt architecture design -- understanding how OpenCode selects different Prompt templates for different LLM Providers, as well as the layered Prompt assembly logic.
