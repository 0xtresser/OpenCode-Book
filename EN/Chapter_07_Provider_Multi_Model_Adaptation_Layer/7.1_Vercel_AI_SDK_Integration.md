# 7.1 Vercel AI SDK Integration

> **Model**: claude-opus-4-6 (anthropic/claude-opus-4-6)
> **Generated on**: 2026-02-17

---

OpenCode needs to interface with over 20 LLM Providers (Anthropic, OpenAI, Google, Azure, AWS Bedrock, etc.), each with its own unique API format, authentication method, and capability profile. Writing separate adapter code for every Provider would result in prohibitively high maintenance costs. OpenCode chose **Vercel AI SDK** (npm package name `ai`) as its unified LLM abstraction layer, building its own adaptation system on top of it.

## 7.1.1 Why Vercel AI SDK?

> **Extended Explanation: Core Concepts of the Vercel AI SDK**
>
> The Vercel AI SDK is an open-source TypeScript library developed by Vercel, designed to provide a unified programming interface for LLM applications. Its core concepts include:
>
> - **`streamText()`**: Streaming text generation — sends messages to an LLM and receives replies as a stream, with support for tool calls.
> - **`generateObject()`**: Structured object generation — instructs the LLM to return a JSON object conforming to a specified Schema.
> - **`LanguageModelV2`**: A unified language model interface — all Provider SDKs implement this interface, so upper-layer code does not need to concern itself with underlying Provider differences.
> - **`Tool`**: Tool definitions — declaratively defines functions the LLM can invoke, including parameter Schemas and execution logic.
> - **`wrapLanguageModel()`**: Model middleware — inserts custom logic before and after LLM calls (e.g., message format transformations).
>
> The design philosophy of the Vercel AI SDK is "write once, run on any Provider" — application code interacts only with the SDK's unified interface, and switching Providers requires only changing the SDK package and configuration.

OpenCode chose the Vercel AI SDK for the following reasons:

1. **Rich Provider ecosystem**: Official and community packages (`@ai-sdk/*`) already exist for 20+ Providers, each implementing the `LanguageModelV2` interface.
2. **Native streaming**: `streamText()` natively supports streaming responses and tool calls, which is critical for real-time CLI output.
3. **Type safety**: Complete TypeScript type definitions that align with OpenCode's Zod-first style.
4. **Middleware support**: `wrapLanguageModel()` allows injecting custom logic without modifying the Provider SDK.
5. **Active maintenance**: As one of Vercel's core open-source projects, it receives frequent updates and rapid bug fixes.

## 7.1.2 Unification through the `LanguageModelV2` Interface

In OpenCode, all Providers are ultimately normalized to the `LanguageModelV2` interface:

```typescript
// Imported from the Vercel AI SDK
import {
  streamText,
  wrapLanguageModel,
  type ModelMessage,
  type StreamTextResult,
  type Tool,
  type ToolSet,
  tool,
  jsonSchema,
} from "ai"
```

`LanguageModelV2` is the universal language model interface defined by the Vercel AI SDK. Whether the underlying model is Claude, GPT, or Gemini, upper-layer code makes calls through the same interface. In `LLM.stream()`, the final call looks like this:

```typescript
return streamText({
  model: wrapLanguageModel({
    model: language,      // LanguageModelV2 instance
    middleware: [/* ... */],
  }),
  messages: [...],
  tools: {...},
  // ...
})
```

This design means that the entire Session, Agent, and Tool systems do not need to know which Provider is being used under the hood — they interact only with the unified interface of `streamText()`. Provider differences are fully encapsulated inside the `provider/` module.

## 7.1.3 Bundled Providers and Dynamic Installation

OpenCode ships with 20 bundled Provider SDKs:

```typescript
const BUNDLED_PROVIDERS: Record<string, (options: any) => SDK> = {
  "@ai-sdk/amazon-bedrock": createAmazonBedrock,
  "@ai-sdk/anthropic": createAnthropic,
  "@ai-sdk/azure": createAzure,
  "@ai-sdk/google": createGoogleGenerativeAI,
  "@ai-sdk/google-vertex": createVertex,
  "@ai-sdk/google-vertex/anthropic": createVertexAnthropic,
  "@ai-sdk/openai": createOpenAI,
  "@ai-sdk/openai-compatible": createOpenAICompatible,
  "@openrouter/ai-sdk-provider": createOpenRouter,
  "@ai-sdk/xai": createXai,
  "@ai-sdk/mistral": createMistral,
  "@ai-sdk/groq": createGroq,
  "@ai-sdk/deepinfra": createDeepInfra,
  "@ai-sdk/cerebras": createCerebras,
  "@ai-sdk/cohere": createCohere,
  "@ai-sdk/gateway": createGateway,
  "@ai-sdk/togetherai": createTogetherAI,
  "@ai-sdk/perplexity": createPerplexity,
  "@ai-sdk/vercel": createVercel,
  "@gitlab/gitlab-ai-provider": createGitLab,
  "@ai-sdk/github-copilot": createGitHubCopilotOpenAICompatible,
}
```

These Providers are **statically imported and bundled**, requiring no additional installation by the user. Each `create*` function accepts configuration options and returns an `SDK` (i.e., `Provider`) instance.

For custom Providers not in the bundled list, OpenCode supports **runtime dynamic installation**:

```typescript
// Dynamic installation logic in getSDK()
const bundledFn = BUNDLED_PROVIDERS[model.api.npm]
if (bundledFn) {
  // Use the bundled Provider
  const loaded = bundledFn({ name: model.providerID, ...options })
  s.sdk.set(key, loaded)
  return loaded
}

// Dynamically install a custom Provider npm package
let installedPath: string
if (!model.api.npm.startsWith("file://")) {
  installedPath = await BunProc.install(model.api.npm, "latest")
} else {
  installedPath = model.api.npm  // Local file protocol
}

const mod = await import(installedPath)
const fn = mod[Object.keys(mod).find((key) => key.startsWith("create"))!]
const loaded = fn({ name: model.providerID, ...options })
```

The dynamic installation flow:

1. Check whether the npm package name is in `BUNDLED_PROVIDERS`.
2. If not, use `BunProc.install()` to install the npm package at runtime.
3. Dynamically load the installed module via `import()`.
4. Automatically locate the factory function by naming convention (an exported function whose name starts with `create`).
5. Call the factory function to create the Provider instance.

This design means users can use **any** npm package that implements the Vercel AI SDK Provider interface — even if OpenCode does not have built-in support, users can specify the npm package name in the configuration file to use it.

---

The next section will dive into the Provider registration and resolution flow — from environment variables to configuration files, from API Keys to OAuth, and how OpenCode unifies Provider information from multiple sources.
