# 7.3 Model Metadata System

> **Model**: claude-opus-4-6 (anthropic/claude-opus-4-6)
> **Generated on**: 2026-02-17

---

Every LLM model has its own unique capability profile — different context window sizes, different input/output modalities (whether it supports images, audio), different pricing, and different reasoning abilities. OpenCode manages all this metadata uniformly through the `Provider.Model` data model, providing decision-making information for upper-layer systems (such as Compaction, permission control, and UI rendering).

## 7.3.1 The `Provider.Model` Type Definition

```typescript
export const Model = z.object({
  id: z.string(),                    // Model ID (e.g., "claude-sonnet-4-20250514")
  providerID: z.string(),            // Owning Provider
  api: z.object({
    id: z.string(),                  // Model ID used in API calls
    url: z.string(),                 // API endpoint URL
    npm: z.string(),                 // SDK npm package name
  }),
  name: z.string(),                  // Model display name
  family: z.string().optional(),     // Model family (e.g., "claude-3")
  capabilities: z.object({
    temperature: z.boolean(),        // Whether the temperature parameter is supported
    reasoning: z.boolean(),          // Whether the model has reasoning capabilities
    attachment: z.boolean(),         // Whether file attachments are supported
    toolcall: z.boolean(),           // Whether tool calls are supported
    input: z.object({                // Input modality support
      text: z.boolean(),
      audio: z.boolean(),
      image: z.boolean(),
      video: z.boolean(),
      pdf: z.boolean(),
    }),
    output: z.object({               // Output modality support
      text: z.boolean(),
      audio: z.boolean(),
      image: z.boolean(),
      video: z.boolean(),
      pdf: z.boolean(),
    }),
    interleaved: z.union([           // Interleaved chain-of-thought support
      z.boolean(),
      z.object({ field: z.enum(["reasoning_content", "reasoning_details"]) }),
    ]),
  }),
  cost: z.object({                   // Pricing (per million tokens)
    input: z.number(),
    output: z.number(),
    cache: z.object({
      read: z.number(),
      write: z.number(),
    }),
    experimentalOver200K: z.object({ /* pricing for context exceeding 200K */ }).optional(),
  }),
  limit: z.object({                  // Limits
    context: z.number(),             // Context window size
    input: z.number().optional(),    // Maximum input token count
    output: z.number(),              // Maximum output token count
  }),
  status: z.enum(["alpha", "beta", "deprecated", "active"]),
  options: z.record(z.string(), z.any()),
  headers: z.record(z.string(), z.string()),
  release_date: z.string(),
  variants: z.record(z.string(), z.record(z.string(), z.any())).optional(),
})
```

### Capability System Design

The `capabilities` field forms the model's **capability matrix**. These boolean flags are used across multiple layers of the system:

| Capability | Use Case |
|------------|----------|
| `temperature` | Whether to pass the temperature parameter during LLM calls |
| `reasoning` | Whether to display chain-of-thought UI, whether to enable variant selection |
| `toolcall` | Whether the model can be used as an Agent (models without tool call support cannot execute the Agentic Loop) |
| `input.image` | Whether users are allowed to send image attachments |
| `input.pdf` | Whether users are allowed to send PDF files |
| `interleaved` | How to handle interleaved chain-of-thought |

The `interleaved` field deserves special attention. It has three possible values:

- `true`: The model natively supports interleaved chain-of-thought (thinking content and text content alternate).
- `false`: Interleaved chain-of-thought is not supported.
- `{ field: "reasoning_content" }`: The model returns thinking content through a special field, requiring format conversion during message processing.

### Purpose of Pricing Information

The `cost` field is not only used for UI display but also affects system behavior:

```typescript
// Logic for the opencode Provider
async opencode(input) {
  if (!hasKey) {
    // When there is no API Key, keep only free models
    for (const [key, value] of Object.entries(input.models)) {
      if (value.cost.input === 0) continue
      delete input.models[key]
    }
  }
  return {
    autoload: Object.keys(input.models).length > 0,
    options: hasKey ? {} : { apiKey: "public" },
  }
}
```

## 7.3.2 models.dev Integration

OpenCode's model database comes from [models.dev](https://models.dev) — a centralized LLM model information service:

```typescript
export namespace ModelsDev {
  export const Data = lazy(async () => {
    // Priority: local cache → build-time snapshot → online fetch
    const file = Bun.file(Flag.OPENCODE_MODELS_PATH ?? filepath)
    const result = await file.json().catch(() => {})
    if (result) return result

    const snapshot = await import("./models-snapshot")
      .then((m) => m.snapshot)
      .catch(() => undefined)
    if (snapshot) return snapshot

    if (Flag.OPENCODE_DISABLE_MODELS_FETCH) return {}
    const json = await fetch(`${url()}/api.json`).then((x) => x.text())
    return JSON.parse(json)
  })
}
```

> **Extended Explanation: The `lazy()` Pattern**
>
> `lazy()` is a deferred computation pattern — the function body executes only on the first invocation, and the result is cached for subsequent use. This is similar to `Instance.state()` but does not depend on a project instance context. This pattern is particularly suited for data that is "loaded once, shared globally."

Data from models.dev is converted from the `ModelsDev.Model` format to the `Provider.Model` format:

```typescript
function fromModelsDevModel(provider: ModelsDev.Provider, model: ModelsDev.Model): Model {
  const m: Model = {
    id: model.id,
    providerID: provider.id,
    name: model.name,
    api: {
      id: model.id,
      url: provider.api!,
      npm: model.provider?.npm ?? provider.npm ?? "@ai-sdk/openai-compatible",
    },
    capabilities: {
      temperature: model.temperature,
      reasoning: model.reasoning,
      input: {
        text: model.modalities?.input?.includes("text") ?? false,
        image: model.modalities?.input?.includes("image") ?? false,
        // ...
      },
      // ...
    },
    cost: {
      input: model.cost?.input ?? 0,
      output: model.cost?.output ?? 0,
      cache: {
        read: model.cost?.cache_read ?? 0,
        write: model.cost?.cache_write ?? 0,
      },
    },
    limit: {
      context: model.limit.context,
      output: model.limit.output,
    },
    // ...
  }

  // Automatically generate variants (e.g., "high", "max" reasoning modes)
  m.variants = mapValues(ProviderTransform.variants(m), (v) => v)
  return m
}
```

### Model Search

OpenCode uses the `fuzzysort` library for fuzzy model searching:

```typescript
const matches = fuzzysort.go(modelID, availableModels, {
  limit: 3,
  threshold: -10000
})
const suggestions = matches.map((m) => m.target)
```

When the user enters a model name with a typo, the system suggests the closest matches, improving the user experience.

## 7.3.3 Small Model Selection

Many auxiliary operations (such as title generation and summary generation) do not require expensive large models. OpenCode automatically selects a smaller model under the same Provider via `getSmallModel()`:

```typescript
export async function getSmallModel(providerID: string) {
  const cfg = await Config.get()

  // User-configured small model takes highest priority
  if (cfg.small_model) {
    const parsed = parseModel(cfg.small_model)
    return getModel(parsed.providerID, parsed.modelID)
  }

  // Search for a small model under the same Provider by priority
  let priority = [
    "claude-haiku-4-5",
    "claude-haiku-4.5",
    "3-5-haiku",
    "3.5-haiku",
    "gemini-3-flash",
    "gemini-2.5-flash",
    "gpt-5-nano",
  ]

  // Special handling for different Providers
  if (providerID.startsWith("opencode")) {
    priority = ["gpt-5-nano"]
  }
  if (providerID.startsWith("github-copilot")) {
    priority = ["gpt-5-mini", "claude-haiku-4.5", ...priority]
  }

  for (const item of priority) {
    for (const model of Object.keys(provider.models)) {
      if (model.includes(item)) return getModel(providerID, model)
    }
  }

  // Ultimate fallback: use the gpt-5-nano provided by opencode
  return getModel("opencode", "gpt-5-nano")
}
```

The small model selection strategy reflects several considerations:

1. **Same Provider preferred**: Prioritize using the same Provider as the primary model to avoid additional API Key configurations.
2. **Cost awareness**: GitHub Copilot users prioritize free models (`gpt-5-mini`).
3. **Fallback strategy**: If no suitable small model is found, use the free `gpt-5-nano` provided by OpenCode.

## 7.3.4 Model Variants

> **Extended Explanation: What Are Model Variants?**
>
> Many modern LLMs support "reasoning modes" — the model can engage in deeper thinking before answering. Different Providers control this feature in different ways: OpenAI uses `reasoningEffort`, Anthropic uses `thinking.budgetTokens`, and Google uses `thinkingConfig`.
>
> OpenCode unifies these differences into an abstraction called "variants" — different runtime configurations of the same model. For example, Claude Sonnet's `high` variant enables a 16,000-token thinking budget, while the `max` variant uses the maximum thinking budget.

Variant generation is handled automatically by the `ProviderTransform.variants()` function:

```typescript
export function variants(model: Provider.Model): Record<string, Record<string, any>> {
  if (!model.capabilities.reasoning) return {}

  switch (model.api.npm) {
    case "@ai-sdk/anthropic":
      return {
        high: {
          thinking: {
            type: "enabled",
            budgetTokens: Math.min(16_000, Math.floor(model.limit.output / 2 - 1)),
          },
        },
        max: {
          thinking: {
            type: "enabled",
            budgetTokens: Math.min(31_999, model.limit.output - 1),
          },
        },
      }

    case "@ai-sdk/openai":
      return Object.fromEntries(
        ["none", "minimal", "low", "medium", "high", "xhigh"]
          .map((effort) => [effort, {
            reasoningEffort: effort,
            reasoningSummary: "auto",
            include: ["reasoning.encrypted_content"],
          }])
      )

    case "@ai-sdk/google":
      return {
        high: { thinkingConfig: { includeThoughts: true, thinkingBudget: 16000 } },
        max: { thinkingConfig: { includeThoughts: true, thinkingBudget: 24576 } },
      }
    // ... more Providers
  }
}
```

The variant system allows users to control complex Provider-specific parameters through simple names (e.g., `"high"`), without needing to understand the API details of each Provider. In Agent configuration, the default variant can be specified via the `variant` field.

---

The next section will analyze the `ProviderTransform` module — how OpenCode handles message format differences, output token limits, and Schema adaptation across different Providers.
