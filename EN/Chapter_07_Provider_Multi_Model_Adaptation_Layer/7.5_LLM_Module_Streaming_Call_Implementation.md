# 7.5 The LLM Module: Streaming Call Implementation

> **Model**: claude-opus-4-6 (anthropic/claude-opus-4-6)
> **Generated on**: 2026-02-17

---

The `LLM` module is the bridge between the Provider layer and the Session layer in OpenCode — it is responsible for transforming the Agent configuration, message history, and tool definitions provided by the upper layer into a complete LLM streaming call. This section provides a full analysis of the `LLM.stream()` implementation, walking through every step from parameter assembly to streaming output.

## 7.5.1 Complete `LLM.stream()` Implementation Analysis

```typescript
export namespace LLM {
  export type StreamInput = {
    user: MessageV2.User        // Current user message
    sessionID: string            // Session ID
    model: Provider.Model        // Model metadata
    agent: Agent.Info            // Agent configuration
    system: string[]             // Additional system instructions
    abort: AbortSignal           // Abort signal
    messages: ModelMessage[]     // Complete conversation history
    small?: boolean              // Whether to use small model configuration
    tools: Record<string, Tool>  // Available tool set
    retries?: number             // Retry count
  }

  export async function stream(input: StreamInput) {
    // ... full implementation
  }
}
```

### Step 1: Fetch Dependent Resources in Parallel

```typescript
const [language, cfg, provider, auth] = await Promise.all([
  Provider.getLanguage(input.model),   // LanguageModelV2 instance
  Config.get(),                         // Global configuration
  Provider.getProvider(input.model.providerID),  // Provider information
  Auth.get(input.model.providerID),     // Authentication information
])
const isCodex = provider.id === "openai" && auth?.type === "oauth"
```

`Promise.all()` is used to load four dependent resources in parallel, avoiding sequential waits. The `isCodex` flag determines whether the current mode is Codex mode (OpenAI's OAuth authentication mode), which has special behavioral handling.

### Step 2: System Prompt Assembly

This part was analyzed in detail in Chapter 6. Here is a brief recap:

```typescript
const system = []
system.push(
  [
    ...(input.agent.prompt
      ? [input.agent.prompt]
      : isCodex ? [] : SystemPrompt.provider(input.model)),
    ...input.system,
    ...(input.user.system ? [input.user.system] : []),
  ]
    .filter((x) => x)
    .join("\n"),
)

// Plugin transformation
await Plugin.trigger("experimental.chat.system.transform",
  { sessionID: input.sessionID, model: input.model }, { system })

// 2-part caching optimization
if (system.length > 2 && system[0] === header) {
  const rest = system.slice(1)
  system.length = 0
  system.push(header, rest.join("\n"))
}
```

### Step 3: Provider Options Merging

```typescript
const variant = !input.small && input.model.variants && input.user.variant
  ? input.model.variants[input.user.variant] : {}

const base = input.small
  ? ProviderTransform.smallOptions(input.model)    // Small model options
  : ProviderTransform.options({                     // Standard options
      model: input.model,
      sessionID: input.sessionID,
      providerOptions: provider.options,
    })

const options = pipe(
  base,                            // Provider default options
  mergeDeep(input.model.options),   // Model-level options
  mergeDeep(input.agent.options),   // Agent-level options
  mergeDeep(variant),               // Variant-level options
)
```

Options merging uses the `pipe` and `mergeDeep` functions from the `remeda` library, forming a clear priority chain: **Provider < Model < Agent < Variant**.

### Step 4: Plugin Parameter Hook

```typescript
const params = await Plugin.trigger(
  "chat.params",
  { sessionID, agent, model, provider, message },
  {
    temperature: input.model.capabilities.temperature
      ? (input.agent.temperature ?? ProviderTransform.temperature(input.model))
      : undefined,
    topP: input.agent.topP ?? ProviderTransform.topP(input.model),
    topK: ProviderTransform.topK(input.model),
    options,
  },
)
```

The `chat.params` hook allows Plugins to modify LLM call parameters at the last moment. oh-my-opencode uses this hook to inject Anthropic's `effort` level (e.g., `budget_tokens`).

### Step 5: Tool Resolution and Permission Filtering

```typescript
const tools = await resolveTools(input)

async function resolveTools(input) {
  const disabled = PermissionNext.disabled(
    Object.keys(input.tools), input.agent.permission)
  for (const tool of Object.keys(input.tools)) {
    if (input.user.tools?.[tool] === false || disabled.has(tool)) {
      delete input.tools[tool]
    }
  }
  return input.tools
}
```

Two layers of filtering ensure the Agent can only "see" tools that are permitted.

### Step 6: LiteLLM Proxy Compatibility

```typescript
if (isLiteLLMProxy && Object.keys(tools).length === 0 && hasToolCalls(input.messages)) {
  tools["_noop"] = tool({
    description: "Placeholder for LiteLLM/Anthropic proxy compatibility",
    inputSchema: jsonSchema({ type: "object", properties: {} }),
    execute: async () => ({ output: "", title: "", metadata: {} }),
  })
}
```

> **Extended Explanation: LiteLLM**
>
> LiteLLM is a popular LLM API proxy that allows calling multiple Providers through a unified interface. However, it has a limitation: if the message history contains tool calls, the request must include at least one tool definition (even if no tools are currently needed). OpenCode works around this limitation by adding a `_noop` placeholder tool that is never actually invoked.

### Step 7: Calling `streamText()`

```typescript
return streamText({
  onError(error) {
    l.error("stream error", { error })
  },

  // Tool call repair
  async experimental_repairToolCall(failed) {
    const lower = failed.toolCall.toolName.toLowerCase()
    if (lower !== failed.toolCall.toolName && tools[lower]) {
      return { ...failed.toolCall, toolName: lower }
    }
    return {
      ...failed.toolCall,
      input: JSON.stringify({ tool: failed.toolCall.toolName, error: failed.error.message }),
      toolName: "invalid",
    }
  },

  temperature: params.temperature,
  topP: params.topP,
  topK: params.topK,
  providerOptions: ProviderTransform.providerOptions(input.model, params.options),
  activeTools: Object.keys(tools).filter((x) => x !== "invalid"),
  tools,
  maxOutputTokens,
  abortSignal: input.abort,
  headers: { /* ... */ },
  maxRetries: input.retries ?? 0,

  messages: [
    ...system.map((x) => ({ role: "system", content: x })),
    ...input.messages,
  ],

  model: wrapLanguageModel({
    model: language,
    middleware: [{
      async transformParams(args) {
        if (args.type === "stream") {
          args.params.prompt = ProviderTransform.message(
            args.params.prompt, input.model, options)
        }
        return args.params
      },
    }],
  }),

  experimental_telemetry: {
    isEnabled: cfg.experimental?.openTelemetry,
    metadata: { userId: cfg.username ?? "unknown", sessionId: input.sessionID },
  },
})
```

### `experimental_repairToolCall`: Tool Call Repair

This is an elegant fault-tolerance mechanism. LLMs sometimes generate incorrect tool names (e.g., wrong casing), and the `experimental_repairToolCall` callback attempts repair when a tool call fails:

1. **Case fix**: If the lowercase version of the tool name exists (e.g., the LLM output `Read` but the actual tool name is `read`), it is automatically corrected.
2. **Invalid tool fallback**: If repair is not possible, the tool call is routed to an `"invalid"` tool, with the error message passed as input — this lets the LLM "see" its own mistake and self-correct.

### `wrapLanguageModel` Middleware

```typescript
model: wrapLanguageModel({
  model: language,
  middleware: [{
    async transformParams(args) {
      if (args.type === "stream") {
        args.params.prompt = ProviderTransform.message(
          args.params.prompt, input.model, options)
      }
      return args.params
    },
  }],
})
```

The middleware executes `ProviderTransform.message()` before messages are sent to the Provider API, performing final format adaptation — including empty message filtering, Tool Call ID normalization, cache marking, and more (see Section 7.4 for details).

## 7.5.2 Authentication System

OpenCode supports two authentication modes:

**API Key Mode**: Users directly provide an API Key, stored via environment variables or the `opencode auth` command.

**OAuth Mode**: Used for services requiring OAuth authorization, such as GitHub Copilot and Codex. The OAuth flow is handled by dedicated Auth Plugins in the Plugin system (e.g., `CopilotAuthPlugin`, `CodexAuthPlugin`).

OAuth mode detection:

```typescript
const isCodex = provider.id === "openai" && auth?.type === "oauth"
```

In Codex mode, the System Prompt is delivered differently — not as a `system` message, but through the `options.instructions` field:

```typescript
if (isCodex) {
  options.instructions = SystemPrompt.instructions()
}
```

## 7.5.3 Request Header Management

```typescript
headers: {
  ...(input.model.providerID.startsWith("opencode")
    ? {
        "x-opencode-project": Instance.project.id,
        "x-opencode-session": input.sessionID,
        "x-opencode-request": input.user.id,
        "x-opencode-client": Flag.OPENCODE_CLIENT,
      }
    : input.model.providerID !== "anthropic"
      ? { "User-Agent": `opencode/${Installation.VERSION}` }
      : undefined),
  ...input.model.headers,
  ...headers,  // Headers injected by Plugins
}
```

Header configuration varies by Provider:

- **OpenCode's own Provider**: Sends tracking information including project ID, session ID, and request ID.
- **Third-party Providers (non-Anthropic)**: Sends a `User-Agent` identifier.
- **Anthropic**: No additional headers are sent (the Anthropic SDK manages its own headers).

## 7.5.4 Small Model Mode

When `input.small` is `true` (used for auxiliary tasks like title generation and summary generation), the system uses streamlined Provider options:

```typescript
export function smallOptions(model: Provider.Model) {
  if (model.providerID === "openai" || model.api.npm === "@ai-sdk/openai") {
    if (model.api.id.includes("gpt-5")) {
      if (model.api.id.includes("5.")) {
        return { store: false, reasoningEffort: "low" }
      }
      return { store: false, reasoningEffort: "minimal" }
    }
    return { store: false }
  }
  if (model.providerID === "google") {
    if (model.api.id.includes("gemini-3")) {
      return { thinkingConfig: { thinkingLevel: "minimal" } }
    }
    return { thinkingConfig: { thinkingBudget: 0 } }
  }
  return {}
}
```

The core idea behind small model mode is to **reduce reasoning depth** — using the lowest reasoningEffort or disabling thinking entirely to achieve faster response times and lower costs.

---

## Chapter Summary

This chapter provided a complete analysis of OpenCode's Provider multi-model adaptation layer:

- **7.1** introduced the core concepts of the Vercel AI SDK and how it is integrated into OpenCode, including 20+ bundled Providers and the runtime dynamic installation mechanism.
- **7.2** provided a detailed analysis of the complete flow from multi-source Provider registration to final state construction — environment variable discovery, configuration file merging, Auth storage, Plugin loading, and Custom Loaders.
- **7.3** examined the model metadata system — the capability matrix, pricing information, the models.dev data source, fuzzy search, and model variants.
- **7.4** dove into `ProviderTransform`'s differentiated adaptation — message format normalization, caching strategies, graceful degradation for unsupported modalities, and SDK key mapping.
- **7.5** fully dissected the `LLM.stream()` implementation — from parameter assembly to streaming calls, from tool call repair to middleware injection.

The design philosophy of the Provider layer can be summarized as: **Unified interfaces hide differences, layered merging provides flexibility, and defensive coding ensures robustness.** Regardless of which Provider and model the user selects, the upper-layer Session, Agent, and Tool systems can operate seamlessly.

In the next chapter, we will explore MCP (Model Context Protocol) — the core protocol through which OpenCode connects to the external tool ecosystem.
