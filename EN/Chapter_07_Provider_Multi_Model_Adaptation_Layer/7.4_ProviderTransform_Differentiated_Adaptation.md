# 7.4 ProviderTransform: Differentiated Model Adaptation

> **Model**: claude-opus-4-6 (anthropic/claude-opus-4-6)
> **Generated on**: 2026-02-17

---

Even with the unified abstraction provided by the Vercel AI SDK, numerous subtle differences remain among different LLM Providers — message format constraints, parameter naming conventions, Schema support levels, caching mechanisms, and more. The `ProviderTransform` module serves as the "adaptation layer" that handles these differences, ensuring that OpenCode's upper-layer logic can switch Providers seamlessly.

## 7.4.1 Message Format Normalization (`normalizeMessages`)

Different Providers impose different constraints on message formats. The `normalizeMessages` function handles these differences:

### Anthropic's Empty Message Restriction

```typescript
if (model.api.npm === "@ai-sdk/anthropic") {
  msgs = msgs
    .map((msg) => {
      if (typeof msg.content === "string") {
        if (msg.content === "") return undefined  // Remove empty string messages
        return msg
      }
      if (!Array.isArray(msg.content)) return msg
      const filtered = msg.content.filter((part) => {
        if (part.type === "text" || part.type === "reasoning") {
          return part.text !== ""  // Remove empty text parts
        }
        return true
      })
      if (filtered.length === 0) return undefined
      return { ...msg, content: filtered }
    })
    .filter((msg) => msg !== undefined && msg.content !== "")
}
```

Anthropic's API rejects messages with empty content. During Compaction, tool outputs may be cleared to empty strings, and this code ensures those empty messages never reach the API.

### Claude's Tool Call ID Normalization

```typescript
if (model.api.id.includes("claude")) {
  return msgs.map((msg) => {
    if (Array.isArray(msg.content)) {
      msg.content = msg.content.map((part) => {
        if ((part.type === "tool-call" || part.type === "tool-result")
            && "toolCallId" in part) {
          return {
            ...part,
            toolCallId: part.toolCallId.replace(/[^a-zA-Z0-9_-]/g, "_"),
          }
        }
        return part
      })
    }
    return msg
  })
}
```

The Claude API requires `toolCallId` to contain only letters, digits, underscores, and hyphens. Other Providers may generate IDs with additional characters, so this code replaces illegal characters with underscores.

### Mistral's Special Requirements

```typescript
if (model.providerID === "mistral" || model.api.id.includes("mistral")) {
  // 1. Tool Call ID must be exactly 9 alphanumeric characters
  const normalizedId = part.toolCallId
    .replace(/[^a-zA-Z0-9]/g, "")
    .substring(0, 9)
    .padEnd(9, "0")

  // 2. Tool messages cannot directly follow User messages; an Assistant message must be inserted
  if (msg.role === "tool" && nextMsg?.role === "user") {
    result.push({
      role: "assistant",
      content: [{ type: "text", text: "Done." }],
    })
  }
}
```

Mistral has two unique constraints:
1. Tool Call IDs must be **exactly 9 alphanumeric characters** (not "at most" but "exactly").
2. In the message sequence, a `tool` message cannot be directly followed by a `user` message — an `assistant` message must be inserted in between as a transition.

### Interleaved Chain-of-Thought Handling

```typescript
if (typeof model.capabilities.interleaved === "object" &&
    model.capabilities.interleaved.field) {
  const field = model.capabilities.interleaved.field
  return msgs.map((msg) => {
    if (msg.role === "assistant" && Array.isArray(msg.content)) {
      const reasoningText = msg.content
        .filter((part) => part.type === "reasoning")
        .map((part) => part.text)
        .join("")
      const filteredContent = msg.content
        .filter((part) => part.type !== "reasoning")

      if (reasoningText) {
        return {
          ...msg,
          content: filteredContent,
          providerOptions: {
            openaiCompatible: { [field]: reasoningText },
          },
        }
      }
    }
    return msg
  })
}
```

Certain models (such as DeepSeek) return chain-of-thought content through special fields (`reasoning_content` or `reasoning_details`) rather than through the standard Vercel AI SDK `reasoning` Part. This code converts back from the standard format to the Provider-specific format when replaying message history.

## 7.4.2 Prompt Caching Strategy (`applyCaching`)

Prompt caching can significantly reduce API call latency and cost. The `applyCaching` function marks cache points for Providers that support caching:

```typescript
function applyCaching(msgs: ModelMessage[], providerID: string): ModelMessage[] {
  // Mark the first 2 system messages and the last 2 non-system messages as cache points
  const system = msgs.filter((msg) => msg.role === "system").slice(0, 2)
  const final = msgs.filter((msg) => msg.role !== "system").slice(-2)

  const providerOptions = {
    anthropic: { cacheControl: { type: "ephemeral" } },
    openrouter: { cacheControl: { type: "ephemeral" } },
    bedrock: { cachePoint: { type: "default" } },
    openaiCompatible: { cache_control: { type: "ephemeral" } },
    copilot: { copilot_cache_control: { type: "ephemeral" } },
  }

  for (const msg of unique([...system, ...final])) {
    msg.providerOptions = mergeDeep(msg.providerOptions ?? {}, providerOptions)
  }
  return msgs
}
```

Key design points of the caching strategy:

1. **System message caching**: System Prompts rarely change throughout a session; caching them avoids reprocessing with every request.
2. **Last two messages caching**: The most recent messages are most likely to be retained in the next conversation turn; caching them improves hit rates.
3. **Multi-Provider compatibility**: Cache option keys are set simultaneously for multiple Providers (`anthropic`, `bedrock`, `copilot`, etc.), ensuring that the cache configuration is correctly recognized regardless of which Provider is in use.

## 7.4.3 Unsupported Modality Handling

When a user sends an attachment type that the current model does not support, the system gracefully degrades:

```typescript
function unsupportedParts(msgs: ModelMessage[], model: Provider.Model): ModelMessage[] {
  return msgs.map((msg) => {
    if (msg.role !== "user" || !Array.isArray(msg.content)) return msg

    const filtered = msg.content.map((part) => {
      if (part.type !== "file" && part.type !== "image") return part

      // Check for empty base64 image data
      if (part.type === "image") {
        const imageStr = part.image.toString()
        if (imageStr.startsWith("data:")) {
          const match = imageStr.match(/^data:([^;]+);base64,(.*)$/)
          if (match && (!match[2] || match[2].length === 0)) {
            return {
              type: "text",
              text: "ERROR: Image file is empty or corrupted.",
            }
          }
        }
      }

      const modality = mimeToModality(mime)
      if (model.capabilities.input[modality]) return part  // Model supports this modality

      // Not supported: convert to error text
      return {
        type: "text",
        text: `ERROR: Cannot read ${name} (this model does not support ${modality} input).`,
      }
    })

    return { ...msg, content: filtered }
  })
}
```

This handling prevents API errors — when a user sends an image to a model that does not support images, the system replaces the image with an error message in text form, allowing the model to inform the user of the limitation.

## 7.4.4 Output Token Limit Management

```typescript
export const OUTPUT_TOKEN_MAX = Flag.OPENCODE_EXPERIMENTAL_OUTPUT_TOKEN_MAX || 32_000

export function maxOutputTokens(model: Provider.Model): number {
  return Math.min(model.limit.output, OUTPUT_TOKEN_MAX) || OUTPUT_TOKEN_MAX
}
```

OpenCode defaults to capping the maximum output for all models at 32,000 tokens. This is a **safety measure** — while some models claim to support longer outputs, excessively long outputs lead to increased latency, higher costs, and degraded quality. Users can adjust this limit via the `OPENCODE_EXPERIMENTAL_OUTPUT_TOKEN_MAX` environment variable.

## 7.4.5 `providerOptions` Key Mapping

Different AI SDK packages use different `providerOptions` key names:

```typescript
function sdkKey(npm: string): string | undefined {
  switch (npm) {
    case "@ai-sdk/github-copilot":              return "copilot"
    case "@ai-sdk/openai":
    case "@ai-sdk/azure":                        return "openai"
    case "@ai-sdk/amazon-bedrock":               return "bedrock"
    case "@ai-sdk/anthropic":
    case "@ai-sdk/google-vertex/anthropic":      return "anthropic"
    case "@ai-sdk/google-vertex":
    case "@ai-sdk/google":                       return "google"
    case "@ai-sdk/gateway":                      return "gateway"
    case "@openrouter/ai-sdk-provider":          return "openrouter"
  }
  return undefined
}
```

When the `providerOptions` key name stored in the message history (based on `providerID`) differs from the key name expected by the SDK, a mapping is required. For example, when using Google Vertex Anthropic, the `providerID` might be `"google-vertex-anthropic"`, but the SDK expects the key `"anthropic"`.

## 7.4.6 Temperature and Sampling Parameter Adaptation per Model

```typescript
export function temperature(model: Provider.Model) {
  const id = model.id.toLowerCase()
  if (id.includes("qwen")) return 0.55
  if (id.includes("claude")) return undefined   // Claude: no default temperature set
  if (id.includes("gemini")) return 1.0
  if (id.includes("glm-4.6")) return 1.0
  if (id.includes("kimi-k2")) {
    if (id.includes("thinking") || id.includes("k2.") || id.includes("k2p")) {
      return 1.0   // Thinking models use high temperature
    }
    return 0.6
  }
  return undefined
}
```

Different models have different sensitivities to the temperature parameter:
- **Claude**: No default value is set (`undefined`); the API uses its own default.
- **Gemini and GLM**: Set to 1.0 — these models perform well at temperature=1.0.
- **Qwen**: Set to 0.55 — an empirically determined value.
- **Reasoning models**: Generally use higher temperatures.

---

The next section will analyze the complete implementation of `LLM.stream()` — from System Prompt assembly to streaming calls, from Provider option injection to error handling.
