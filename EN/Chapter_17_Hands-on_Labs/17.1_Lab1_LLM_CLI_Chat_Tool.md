# 17.1 Lab 1: Building a Simple LLM CLI Chat Tool

> **Model**: claude-opus-4-6 (anthropic/claude-opus-4-6)
> **Generation Date**: 2026-02-18

---

This chapter walks readers through five progressive labs to build the key components of an AI programming assistant from scratch. Each lab directly corresponds to the OpenCode source code analyzed in earlier chapters — Lab 1 maps to the Session/LLM module in Chapter 4, Lab 2 maps to the Tool system in Chapter 5, Lab 3 maps to MCP in Chapter 8, Lab 4 maps to the Plugin system in Chapter 13, and Lab 5 maps to the oh-my-opencode multi-Agent architecture in Chapter 15.

## 17.1.1 Objective

Build a terminal chat program based on the Vercel AI SDK. Upon completion, you will have a command-line tool capable of multi-turn, streaming conversations with an LLM — this is the most fundamental core capability of OpenCode.

In the OpenCode source code, `session/llm.ts` is the core module that uses the Vercel AI SDK's `streamText` function to communicate with an LLM. Our lab will reproduce this core logic.

> **Extended Explanation — Vercel AI SDK**
>
> The Vercel AI SDK (npm package name: `ai`) is one of the most popular AI application development frameworks in the JavaScript/TypeScript ecosystem. It provides a unified interface for calling different LLM providers (OpenAI, Anthropic, Google, etc.), with core features including:
> - `streamText()`: Streaming text generation
> - `generateText()`: One-shot text generation
> - `tool()`: Tool/function definition
> - A unified message format (`CoreMessage`)
>
> OpenCode chose the Vercel AI SDK over calling each provider's API directly precisely because of its **provider abstraction layer** — the same code can seamlessly switch between different LLMs.

## 17.1.2 Implementation Steps

### Step 1: Initialize the Project

```bash
# Create project directory
mkdir my-ai-cli && cd my-ai-cli

# Initialize a Bun project (you can also use npm init)
bun init -y

# Install dependencies
bun add ai @ai-sdk/openai
```

`ai` is the core Vercel AI SDK package, and `@ai-sdk/openai` is the OpenAI-compatible provider adapter (it also supports calling other services that are compatible with the OpenAI API format).

### Step 2: Implement Basic Conversation

Create `index.ts`:

```typescript
import { streamText } from "ai"
import { openai } from "@ai-sdk/openai"
import * as readline from "readline"

// Create model instance
const model = openai("gpt-4o-mini")

// Message history — the core data structure for multi-turn conversation
const messages: Array<{ role: "user" | "assistant"; content: string }> = []

// Create terminal input interface
const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
})

async function chat(userInput: string) {
  // Add user message to history
  messages.push({ role: "user", content: userInput })

  // Call LLM and get streaming response
  const result = streamText({
    model,
    messages,
  })

  // Stream output
  let fullResponse = ""
  process.stdout.write("\nAssistant: ")

  for await (const chunk of result.textStream) {
    process.stdout.write(chunk)
    fullResponse += chunk
  }

  console.log("\n")

  // Add assistant reply to history
  messages.push({ role: "assistant", content: fullResponse })
}

// Main loop
function prompt() {
  rl.question("You: ", async (input) => {
    if (input.toLowerCase() === "exit") {
      rl.close()
      return
    }
    await chat(input)
    prompt()
  })
}

console.log("AI CLI Chat Tool (type exit to quit)")
console.log("=" .repeat(40))
prompt()
```

### Step 3: Run

```bash
# Set API Key
export OPENAI_API_KEY="sk-your-key-here"

# Run
bun run index.ts
```

At this point you should see an interaction similar to this:

```
AI CLI Chat Tool (type exit to quit)
========================================
You: Hello, please introduce yourself
Assistant: Hello! I'm an AI assistant that can help you answer questions...

You: What did you say in your last message?
Assistant: I just said "Hello! I'm an AI assistant..."
```

Notice the second turn — the LLM is able to recall the content from the previous turn, which proves that the `messages` array is correctly maintaining the conversation history.

### Step 4: Add a System Prompt

OpenCode uses carefully designed System Prompts to define Agent behavior. Let's add one to our CLI tool as well:

```typescript
const result = streamText({
  model,
  system: `You are a professional programming assistant.
Rules:
1. Answers should be concise and accurate
2. Code should include comments
3. If you are unsure, state your uncertainty rather than fabricating an answer`,
  messages,
})
```

The `system` parameter corresponds to `session/system.ts` and `agent/prompt/*.txt` in OpenCode — defining the Agent's role, capability boundaries, and behavioral rules through the System Prompt.

## 17.1.3 Key Concepts Summary

### LLM API Calls

Interaction with an LLM is essentially an HTTP POST request. The Vercel AI SDK encapsulates this process, but what happens under the hood is:

```
POST https://api.openai.com/v1/chat/completions
Content-Type: application/json
Authorization: Bearer sk-xxx

{
  "model": "gpt-4o-mini",
  "messages": [
    {"role": "system", "content": "You are a programming assistant"},
    {"role": "user", "content": "What is recursion?"},
    {"role": "assistant", "content": "Recursion is when a function calls itself..."},
    {"role": "user", "content": "Give me an example"}
  ],
  "stream": true
}
```

LLMs are **stateless** — every request must include the complete message history. The model itself does not remember previous conversations; "memory" depends entirely on the `messages` array maintained by the client. This is the core problem that OpenCode's Session module addresses.

### Streaming

`streamText()` returns an async iterable (AsyncIterable), where the LLM's response arrives incrementally in small chunks. Streaming has two key advantages:

1. **Low perceived latency**: Users can see output as the model generates it, rather than waiting for the complete response
2. **Memory efficiency**: There is no need to load the entire response into memory at once

In OpenCode, the `stream()` function in `session/llm.ts` also returns a `StreamTextResult`, and TUI components listen to events to update the interface in real time.

### Message History

The `messages` array is the core of multi-turn conversation. Each message contains a `role` and `content`:

| role | Meaning | Source |
|------|---------|--------|
| `system` | System instructions | Defined by developer |
| `user` | User input | User |
| `assistant` | AI reply | Generated by LLM |

In OpenCode, message history is persisted to local storage (`~/.opencode/sessions/`), so users can resume previous conversations after closing and reopening the terminal. Our lab currently only stores messages in memory — persistence will be implemented progressively in subsequent labs.

### Mapping to OpenCode Source Code

| Lab Component | OpenCode Counterpart |
|---------------|----------------------|
| `messages` array | `session/message-v2.ts` message system |
| `streamText()` call | `stream()` function in `session/llm.ts` |
| System Prompt | `session/system.ts` + `agent/prompt/*.txt` |
| Model selection | Provider system in `provider/provider.ts` |
| readline interaction | TUI system in `cli/cmd/tui/` |

In the next lab, we will add the most critical capability to this CLI tool — Tool Use, enabling the AI to read and write files and execute system commands.
