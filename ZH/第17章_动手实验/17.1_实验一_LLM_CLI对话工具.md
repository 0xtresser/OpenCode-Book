# 17.1 实验一：实现一个简单的 LLM CLI 对话工具

> **模型**: claude-opus-4-6 (anthropic/claude-opus-4-6)  
> **生成日期**: 2026-02-18

---

本章将通过五个递进式实验，带领读者从零开始构建一个 AI 编程助手的关键组件。每个实验都与前面章节分析的 OpenCode 源码直接对应——实验一对应第 4 章的 Session/LLM 模块，实验二对应第 5 章的 Tool 系统，实验三对应第 8 章的 MCP，实验四对应第 13 章的 Plugin 系统，实验五对应第 15 章的 oh-my-opencode 多 Agent 架构。

## 17.1.1 目标

搭建一个基于 Vercel AI SDK 的终端对话程序。完成后，你将拥有一个可以与 LLM 进行多轮流式对话的命令行工具——这正是 OpenCode 最底层的核心能力。

在 OpenCode 源码中，`session/llm.ts` 就是使用 Vercel AI SDK 的 `streamText` 函数与 LLM 通信的核心模块。我们的实验将还原这个核心逻辑。

> **衍生解释——Vercel AI SDK**
>
> Vercel AI SDK（npm 包名为 `ai`）是目前 JavaScript/TypeScript 生态中最流行的 AI 应用开发框架之一。它提供了统一的接口来调用不同的 LLM 提供商（OpenAI、Anthropic、Google 等），核心功能包括：
> - `streamText()`：流式文本生成
> - `generateText()`：一次性文本生成
> - `tool()`：工具/函数定义
> - 统一的消息格式（`CoreMessage`）
>
> OpenCode 选择 Vercel AI SDK 而非直接调用各家 API，正是看中了它的**提供商抽象层**——相同的代码可以无缝切换不同的 LLM。

## 17.1.2 实现步骤

### 步骤一：初始化项目

```bash
# 创建项目目录
mkdir my-ai-cli && cd my-ai-cli

# 初始化 Bun 项目（也可以用 npm init）
bun init -y

# 安装依赖
bun add ai @ai-sdk/openai
```

`ai` 是 Vercel AI SDK 的核心包，`@ai-sdk/openai` 是 OpenAI 兼容的提供商适配器（也支持调用其他兼容 OpenAI API 格式的服务）。

### 步骤二：实现基础对话

创建 `index.ts`：

```typescript
import { streamText } from "ai"
import { openai } from "@ai-sdk/openai"
import * as readline from "readline"

// 创建模型实例
const model = openai("gpt-4o-mini")

// 消息历史——这是多轮对话的核心数据结构
const messages: Array<{ role: "user" | "assistant"; content: string }> = []

// 创建终端输入接口
const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
})

async function chat(userInput: string) {
  // 将用户消息加入历史
  messages.push({ role: "user", content: userInput })

  // 调用 LLM，获取流式响应
  const result = streamText({
    model,
    messages,
  })

  // 流式输出
  let fullResponse = ""
  process.stdout.write("\nAssistant: ")

  for await (const chunk of result.textStream) {
    process.stdout.write(chunk)
    fullResponse += chunk
  }

  console.log("\n")

  // 将助手回复加入历史
  messages.push({ role: "assistant", content: fullResponse })
}

// 主循环
function prompt() {
  rl.question("You: ", async (input) => {
    if (input.toLowerCase() === "exit") {
      rl.close()
      return
    }
    await chat(input)
    prompt()
  })
}

console.log("AI CLI 对话工具（输入 exit 退出）")
console.log("=" .repeat(40))
prompt()
```

### 步骤三：运行

```bash
# 设置 API Key
export OPENAI_API_KEY="sk-your-key-here"

# 运行
bun run index.ts
```

此时你应该能看到类似这样的交互：

```
AI CLI 对话工具（输入 exit 退出）
========================================
You: 你好，请介绍一下自己
Assistant: 你好！我是一个 AI 助手，可以帮助你回答问题...

You: 上一句话你说了什么？
Assistant: 我刚才说的是"你好！我是一个 AI 助手..."
```

注意第二轮对话——LLM 能够回忆上一轮的内容，这证明 `messages` 数组正确地维护了对话历史。

### 步骤四：添加 System Prompt

OpenCode 使用精心设计的 System Prompt 来定义 Agent 的行为。让我们给 CLI 工具也添加一个：

```typescript
const result = streamText({
  model,
  system: `你是一个专业的编程助手。
规则：
1. 回答要简洁准确
2. 代码要附带注释
3. 如果不确定，说明你不确定而不是编造答案`,
  messages,
})
```

`system` 参数对应 OpenCode 中 `session/system.ts` 和 `agent/prompt/*.txt` 的功能——通过 System Prompt 定义 Agent 的角色、能力边界和行为规范。

## 17.1.3 知识点总结

### LLM API 调用

与 LLM 的交互本质上是一次 HTTP POST 请求。Vercel AI SDK 封装了这个过程，但底层发生的是：

```
POST https://api.openai.com/v1/chat/completions
Content-Type: application/json
Authorization: Bearer sk-xxx

{
  "model": "gpt-4o-mini",
  "messages": [
    {"role": "system", "content": "你是一个编程助手"},
    {"role": "user", "content": "什么是递归？"},
    {"role": "assistant", "content": "递归是函数调用自身..."},
    {"role": "user", "content": "给个例子"}
  ],
  "stream": true
}
```

LLM 是**无状态**的——每次请求都需要发送完整的消息历史。模型本身不记住之前的对话，"记忆"完全依赖客户端维护的 `messages` 数组。这就是 OpenCode 的 Session 模块要解决的核心问题。

### 流式处理（Streaming）

`streamText()` 返回的是一个异步可迭代对象（AsyncIterable），LLM 的回复以小块（chunk）的形式逐步到达。流式处理有两个关键优势：

1. **低延迟感知**：用户可以在模型生成的同时看到输出，而不是等待完整回复
2. **内存效率**：不需要一次性加载完整响应到内存

在 OpenCode 中，`session/llm.ts` 的 `stream()` 函数返回的也是 `StreamTextResult`，TUI 组件通过监听事件来实时更新界面。

### 消息历史

`messages` 数组是多轮对话的核心。每条消息包含 `role`（角色）和 `content`（内容）：

| role | 含义 | 来源 |
|------|------|------|
| `system` | 系统指令 | 开发者定义 |
| `user` | 用户输入 | 用户 |
| `assistant` | AI 回复 | LLM 生成 |

在 OpenCode 中，消息历史被持久化到本地存储（`~/.opencode/sessions/`），这样用户关闭终端后重新打开可以恢复之前的对话。我们的实验目前只保存在内存中——持久化将在后续实验中逐步实现。

### 与 OpenCode 源码的对应

| 实验组件 | OpenCode 对应 |
|----------|---------------|
| `messages` 数组 | `session/message-v2.ts` 消息系统 |
| `streamText()` 调用 | `session/llm.ts` 的 `stream()` 函数 |
| System Prompt | `session/system.ts` + `agent/prompt/*.txt` |
| 模型选择 | `provider/provider.ts` 的 Provider 系统 |
| readline 交互 | `cli/cmd/tui/` 的 TUI 系统 |

下一个实验中，我们将为这个 CLI 工具添加最关键的能力——Tool Use（工具调用），让 AI 能够读写文件和执行系统命令。
